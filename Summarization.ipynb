{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Package Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers datasets evaluate rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets evaluate transformers[sentencepiece]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git-lfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "ds_train = load_dataset(\"csebuetnlp/xlsum\", \"amharic\", split=\"train[:4000]\")\n",
    "en_train=load_dataset(\"csebuetnlp/xlsum\", \"english\", split=\"train[:4000]\")\n",
    "french_train=load_dataset(\"csebuetnlp/xlsum\", \"french\", split=\"train[:4000]\")\n",
    "kor_train=load_dataset(\"csebuetnlp/xlsum\", \"korean\", split=\"train[:4000]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_val = load_dataset(\"csebuetnlp/xlsum\", \"amharic\", split=\"validation[:500]\")\n",
    "en_val=load_dataset(\"csebuetnlp/xlsum\", \"english\", split=\"validation[:500]\")\n",
    "french_val=load_dataset(\"csebuetnlp/xlsum\", \"french\", split=\"validation[:500]\")\n",
    "kor_val=load_dataset(\"csebuetnlp/xlsum\", \"korean\", split=\"validation[:500]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_test = load_dataset(\"csebuetnlp/xlsum\", \"amharic\", split=\"test[:500]\")\n",
    "en_test=load_dataset(\"csebuetnlp/xlsum\", \"english\", split=\"test[:500]\")\n",
    "french_test=load_dataset(\"csebuetnlp/xlsum\", \"french\", split=\"test[:500]\")\n",
    "kor_test=load_dataset(\"csebuetnlp/xlsum\", \"korean\", split=\"test[:500]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds= DatasetDict({'train':ds_train, 'validation':ds_val, 'test':ds_test})\n",
    "en=DatasetDict({'train':en_train, 'validation':en_val, 'test':en_test})\n",
    "kor=DatasetDict({'train':kor_train, 'validation':kor_val, 'test':kor_test})\n",
    "french=DatasetDict({'train':french_train, 'validation':french_val, 'test':french_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'url', 'title', 'summary', 'text'],\n",
       "        num_rows: 4000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'url', 'title', 'summary', 'text'],\n",
       "        num_rows: 500\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'url', 'title', 'summary', 'text'],\n",
       "        num_rows: 500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'url', 'title', 'summary', 'text'],\n",
       "        num_rows: 4000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'url', 'title', 'summary', 'text'],\n",
       "        num_rows: 500\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'url', 'title', 'summary', 'text'],\n",
       "        num_rows: 500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "french"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'url', 'title', 'summary', 'text'],\n",
       "        num_rows: 4000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'url', 'title', 'summary', 'text'],\n",
       "        num_rows: 500\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'url', 'title', 'summary', 'text'],\n",
       "        num_rows: 500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Korean Dataset\n",
    "kor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'url', 'title', 'summary', 'text'],\n",
       "    num_rows: 4000\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kor['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'>> Title: 코로나19 : 영국 종교 단체들...예배 금지 조치 비판'\n",
      "'>> Summary: 신종 코로나바이러스 감염증(코로나19) 확산을 막기 위해 5일부터 2차 봉쇄 조치에 들어가는 영국에서는 공동 예배 금지 지침이 내려졌다. 이와 관련해 종교 단체들은 새로운 금지 규정을 강하게 비판하고 있다.'\n",
      "\n",
      "'>> Title: 스탠리: '마블의 아버지'...그는 어떤 사람이었나'\n",
      "'>> Summary: 미국의 만화 작가이자 마블 시리즈의 전설로 유명한 스탠 리가 만 95세의 나이로 사망했다.'\n",
      "\n",
      "'>> Title: 중국: '하루 330원'으로 살아온 중국 대학생 사망'\n",
      "'>> Summary: 남동생을 부양하기 위해 지난 수년간 하루에 2위안(약 330원)의 생활비로 살아온 중국의 한 대학생이 영양실조로 사망했다고 중국 현지 언론이 보도했다.'\n",
      "\n",
      "'>> Title: [BBC 라디오] 브라질, 낙태금지법 속 번지는 '자가 낙태' 시도'\n",
      "'>> Summary: 아래는 영국 공영방송 BBC 뉴스의 한국어 라디오, BBC 코리아 방송의 2018년 6월 27일 보도입니다.'\n"
     ]
    }
   ],
   "source": [
    "def show_samples(dataset, num_samples=4, seed=100):\n",
    "    sample = dataset[\"train\"].shuffle(seed=seed).select(range(num_samples))\n",
    "    for example in sample:\n",
    "        print(f\"\\n'>> Title: {example['title']}'\")\n",
    "        print(f\"'>> Summary: {example['summary']}'\")\n",
    "\n",
    "\n",
    "show_samples(kor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'>> Title: Where the Brexiteers went: Johnson, Davis and Fox mapped'\n",
      "'>> Summary: They're known as the Three Brexiteers - the ministers who all campaigned to leave the EU and are now shaping the UK's foreign relations for years to come. Boris Johnson, David Davis and Liam Fox have been racking up the air miles to put the UK's case...'\n",
      "\n",
      "'>> Title: Nato's crisis of trust in Afghanistan'\n",
      "'>> Summary: The man blamed for killing two Nato officials inside the Afghan interior ministry at the weekend should never have been given security clearance, the BBC has learned. A catalogue of security blunders led to the shootings and his escape. There are now real concerns for the future of the relationship between Nato and its Afghan security partners, Bilal Sarwary reports from Kabul.'\n",
      "\n",
      "'>> Title: Florida school shooting: A survivor's story'\n",
      "'>> Summary: It was 14:30 when David Hogg heard the first shot.'\n",
      "\n",
      "'>> Title: No defence witnesses called in Craig Whyte fraud trial'\n",
      "'>> Summary: Former Rangers owner Craig Whyte's defence team have closed their case in his fraud trial after deciding not to call any witnesses.'\n"
     ]
    }
   ],
   "source": [
    "show_samples(en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'>> Title: በቻይና ባለመስታወት ድልደይ ላይ ሲጓዝ የነበረ ግለሰብ ካጋጠመው አደጋ ተንጠልጥሎ ተረፈ'\n",
      "'>> Summary: በቻይና ባለመስታወት ድልደይ ላይ ሲጓዝ የነበረ ግለሰብ ካጋጠመው አደጋ ተንጠልጥሎ ተረፈ'\n",
      "\n",
      "'>> Title: ሲዳማ: 10ኛው የኢትዮጵያ ክልል'\n",
      "'>> Summary: የሲዳማ ዞን ቀጣይ ዕጣ ፈንታን ለመወሰን ረቡዕ ዕለት በተካሄደው ሕዝበ ውሳኔ ዞኑ የአገሪቱ 10ኛው ክልል የመሆን ድምጽ ማግኘቱን የብሔራዊ ምርጫ ቦርድ አስታውቋል።'\n",
      "\n",
      "'>> Title: አፕል ቲቪ፡ አዲሱ የበይነ መረብ ሥርጭቱን ይፋ አደረገ'\n",
      "'>> Summary: ዝነኛ ሰዎች በተካፈሉበት ካሊፎረኒያ ውስጥ በተደረገ ደማቅ ዝግጅት አፕል 'አፕል ቲቪ' የተሰኘውን አዲስ የቴሌቪዥን ስርጭት መጀመሩን ይፋ አደረገ።'\n",
      "\n",
      "'>> Title: ሃሪ እና ሜጋን ልጃቸውን በድሮን ፎቶ ያነሳው ግለሰብ ላይ ክስ መሰረቱ'\n",
      "'>> Summary: የሰሴክሱ መስፍን ሃሪ እና ባለቤቱ ሜጋን በድሮን አማካኝነት ጨቅላ ልጃቸው አርቺን ፎቶ ያነሳው ግለሰብ ላይ ክስ መሰረቱ።'\n"
     ]
    }
   ],
   "source": [
    "show_samples(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'>> Title: Pologne : le chef du gouvernement victime d'accident'\n",
      "'>> Summary: La Première ministre polonaise, Beata Szydlo, se trouve dans un état stable à l'hôpital où elle a été admise après un accident de voiture, selon le gouvernement.'\n",
      "\n",
      "'>> Title: Pourquoi nous allons vers une pénurie de caoutchouc, cet étonnant matériau nécessaire ?'\n",
      "'>> Summary: Le caoutchouc naturel est un matériau exceptionnellement solide, flexible et extrêmement étanche.'\n",
      "\n",
      "'>> Title: \"Game of Thrones\" triomphe aux Emmys'\n",
      "'>> Summary: \"Game of Thrones\" a battu des records aux Emmys, les \"Oscars de la télévision\", dont la cérémonie a eu lieu ce dimanche, à Los Angeles, aux Etats-Unis.'\n",
      "\n",
      "'>> Title: La galère des étudiants boursiers congolais à l'étranger'\n",
      "'>> Summary: Cuba, Sénégal, Côte d'Ivoire, Russie... Un peu partout dans le monde, les étudiants congolais se mobilisent pour obtenir le versement du quatrième trimestre de leur bourse au titre de 2016.'\n"
     ]
    }
   ],
   "source": [
    "show_samples(french)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join The dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'>> Title: L'incroyable histoire des \"dribbleurs de l'indépendance\" de l'Algérie'\n",
      "'>> Summary: En 1958, à la veille de la Coupe du monde, plusieurs joueurs franco-algériens évoluant dans les plus grands clubs français et sélectionnés dans l'équipe de France, font défection et rejoignent Tunis où ils forment l'équipe de football du FLN, le mouvement indépendantiste algérien. Ils vont écrire les plus belles pages de l'histoire du football algérien. Voici leur histoire.'\n",
      "\n",
      "'>> Title: 유엔 보고서: 북한을 떠났다가 잡혀들어간 여성들이 겪는 인권 침해'\n",
      "'>> Summary: 탈북했다가 북한으로 다시 강제송환된 여성들이 조사 과정에서 구타는 물론, 항문과 질 검사를 남성 보안원에 의해 받는 경우도 있었다는 내용의 유엔 보고서가 발표됐다.'\n",
      "\n",
      "'>> Title: ኮሮና ቫይረስ፡የቤት እንስሳት ኮሮናን ያመጡብናል ብላችሁ እንዳትሰጉ ተባለ'\n",
      "'>> Summary: በዙሪያችን ያለው ሁሉ ስጋት የሚያጭር እንጂ ልብን የሚያሳርፍ አይመስልም። በተለይ በዚህ በኮሮና ወረርሽኝ ዘመን ትክክለኛ መረጃ ከሌለዎች ሁሉም ነገር ስጋት ያጭራል።'\n",
      "\n",
      "'>> Title: ኮሮናቫይረስ፡ የእንቅስቃሴ ገደብና የታዳጊዎች የአእምሮ ጤና'\n",
      "'>> Summary: በርካታ አገራት የኮሮናቫይረስ ወረርሽኝ ለመከላከል ብሎም ለመቆጣጠር በማሰብ ይጠቅመናል ያሉትን እርምጃ እየወሰዱ ይገኛሉ። የሰዎች እንቅስቃሴን በመገደብ ንክኪ እንዲቀንሱ ማድረግ ደግሞ የመጀመሪያው ነው።'\n"
     ]
    }
   ],
   "source": [
    "#Concatenating The training datasets of all the four languages\n",
    "from datasets import concatenate_datasets, DatasetDict\n",
    "\n",
    "comp_dataset = DatasetDict()\n",
    "\n",
    "for split in en.keys():\n",
    "    comp_dataset[split] = concatenate_datasets(\n",
    "        [en[split], french[split], ds[split], kor[split]]\n",
    "    )\n",
    "    comp_dataset[split] = comp_dataset[split].shuffle(seed=32)\n",
    "\n",
    "# Peek at a few examples\n",
    "show_samples(comp_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def train(model, epochs, train_dataloader,val_dataloader):\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "    # Fine-tuning loop\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in train_dataloader:\n",
    "            optimizer.zero_grad()\n",
    "    \n",
    "            # Move input and labels to device (GPU/CPU)\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "    \n",
    "            # Forward pass\n",
    "            outputs = model(input_ids=input_ids, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Training Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_dataloader:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "                \n",
    "    \n",
    "                outputs = model(input_ids=input_ids, labels=labels)\n",
    "                loss = outputs.loss\n",
    "                val_loss += loss.item()\n",
    "    \n",
    "        avg_val_loss = val_loss / len(val_dataloader)\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(text, tokenizer, model):\n",
    "    input_ids = tokenizer.encode(text, return_tensors=\"pt\", max_length=512, truncation=True).to(device)\n",
    "    summary_ids = model.generate(input_ids, max_length=128, min_length=10, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home1/shreeyagarg/anaconda3/lib/python3.11/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "2024-09-28 18:06:09.866091: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-28 18:06:09.886167: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-28 18:06:09.893003: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-28 18:06:09.909175: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-28 18:06:10.990279: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/data/home1/shreeyagarg/anaconda3/lib/python3.11/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/data/home1/shreeyagarg/anaconda3/lib/python3.11/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred, tokenizer):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "\n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mT5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home1/shreeyagarg/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/data/home1/shreeyagarg/anaconda3/lib/python3.11/site-packages/transformers/modeling_utils.py:519: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=map_location)\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "from transformers import MT5ForConditionalGeneration, MT5Tokenizer\n",
    "model_t5 = MT5ForConditionalGeneration.from_pretrained('google/mt5-small')\n",
    "tokenizer_t5 = MT5Tokenizer.from_pretrained('google/mt5-small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'url', 'title', 'summary', 'text'],\n",
       "        num_rows: 16000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'url', 'title', 'summary', 'text'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'url', 'title', 'summary', 'text'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comp_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(example):\n",
    "    # Input: review body, Target: review summary\n",
    "    input_text = example['text']\n",
    "    target_text = example['summary']\n",
    "\n",
    "    # Tokenize the inputs and summaries\n",
    "    input_ids = tokenizer_t5(input_text, padding='max_length', truncation=True, max_length=512, return_tensors=\"pt\").input_ids\n",
    "    labels = tokenizer_t5(target_text, padding='max_length', truncation=True, max_length=150, return_tensors=\"pt\").input_ids\n",
    "    labels[labels == tokenizer_t5.pad_token_id] = -100  # Replace padding tokens with -100 for loss calculation\n",
    "\n",
    "    return {'input_ids': input_ids, 'labels': labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "667047e955694050b68bec890b0469a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "processed_data_train=comp_dataset['train'].map(preprocess_data, batched=True)\n",
    "processed_data_train.set_format(type='torch', columns=['input_ids', 'labels'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a217ae6fd3d4d7db7e186d8a26faac4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preprocess_data_val=comp_dataset['validation'].map(preprocess_data, batched=True)\n",
    "preprocess_data_val.set_format(type='torch', columns=['input_ids', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer_t5, return_tensors='pt') # returns tensorflow vectors\n",
    "train_data = DataLoader(\n",
    "   processed_data_train,\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator\n",
    ")\n",
    "val_data=DataLoader(\n",
    "   preprocess_data_val,\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   381,    666,  20521,  ...,    261,  17280,      1],\n",
      "        [  1794,   2856,  23937,  ...,    260,    313,      1],\n",
      "        [  1237,  22984,   1557,  ...,   3858,  35524,      1],\n",
      "        ...,\n",
      "        [  6424,    262, 138816,  ...,      0,      0,      0],\n",
      "        [   259, 105015,   6074,  ...,   2363,   4420,      1],\n",
      "        [  1916,   1156,   1371,  ...,      0,      0,      0]])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_data:\n",
    "    print(batch['input_ids'])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model_t5, 25, train_data, val_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mBart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home1/shreeyagarg/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/data/home1/shreeyagarg/anaconda3/lib/python3.11/site-packages/transformers/modeling_utils.py:519: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=map_location)\n"
     ]
    }
   ],
   "source": [
    "from transformers import MBartForConditionalGeneration,  MBart50TokenizerFast\n",
    "\n",
    "model = MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-50\")\n",
    "tokenizer_mbart = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(example):\n",
    "    # Input: review body, Target: review summary\n",
    "    input_text = example['text']\n",
    "    target_text = example['summary']\n",
    "\n",
    "    # Tokenize the inputs and summaries\n",
    "    input_ids = tokenizer_mbart(input_text, padding='max_length', truncation=True, max_length=512, return_tensors=\"pt\").input_ids\n",
    "    labels = tokenizer_mbart(target_text, padding='max_length', truncation=True, max_length=150, return_tensors=\"pt\").input_ids\n",
    "    labels[labels == tokenizer_mbart.pad_token_id] = -100  # Replace padding tokens with -100 for loss calculation\n",
    "\n",
    "    return {'input_ids': input_ids, 'labels': labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41b033fd44e948b1871e59337c0f068c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "processed_data_train=comp_dataset['train'].map(preprocess_data, batched=True)\n",
    "processed_data_train.set_format(type='torch', columns=['input_ids', 'labels'])\n",
    "preprocess_data_val=comp_dataset['validation'].map(preprocess_data, batched=True)\n",
    "preprocess_data_val.set_format(type='torch', columns=['input_ids', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer_mbart, return_tensors='pt') # returns tensorflow vectors\n",
    "train_data1= DataLoader(\n",
    "   processed_data_train,\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator\n",
    ")\n",
    "val_data1=DataLoader(\n",
    "   preprocess_data_val,\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model, 15, train_data1, val_data1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
